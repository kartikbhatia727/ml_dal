{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "A0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWgBoYfGbGLc"
      },
      "source": [
        "<center><h1> Assignment 0 </h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6gwTaw5bGLd"
      },
      "source": [
        "This assignment is designed to confirm your background knowledge in the fundamentals of Linear Algebra, Probability, and Fully-connected Neural Networks. All the imports made in this notebook are as below; if these imports work, you are (mostly) set to complete the assignment. You are encouraged to read/view optional materials as needed, and some information will be provided in the upcoming programming review (Friday)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziLyp__TbGLe"
      },
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upOd2gypb7p-"
      },
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td>**Section**</td>\n",
        "    <td>**Score**</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> Multivariate Gaussians <br> [Q1 + Q2 + Q3] </td>\n",
        "    <td> 3 + 3 + 4 </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> Invertible Neural Network <br> [forward + invert + bonus] </td>\n",
        "    <td> 2 + 5 + 5</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td> SGD </td>\n",
        "    <td> 3 </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **Total**  </td>\n",
        "    <td> **20** <br>[+5 bonus] </td>\n",
        "  </tr>\n",
        "  </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFVeD5gRbGLi"
      },
      "source": [
        "# I. Multivariate Gaussians"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzsaIeAmbGLi"
      },
      "source": [
        "In the following, we will refer to the matrix $\\mathbf{A}_{M\\times M}$ and the column vectors $\\mathbf{y}_{M\\times 1}$, $\\mathbf{x}_{M\\times 1}$ and $\\mu_{M\\times 1}$.\n",
        "\n",
        "\n",
        "[Q1] If $\\mathbf{y} = \\mathbf{A}\\mathbf{x} + \\mu$ and $\\mathbf{x} \\sim N(0,I_M)$ show that:\n",
        "\n",
        "1. $\\mathbb{E}[\\mathbf{y}]=\\mu$ \n",
        "2. $\\mathbb{E}[\\left(\\mathbf{y-\\mu}\\right)\\left(\\mathbf{y-\\mu}\\right)^\\top]=\\mathbf{A}\\mathbf{A}^\\top$.\n",
        "\n",
        "[Q2] Fill out the following functions for confirming the above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEj6KWsz0V9Z"
      },
      "source": [
        "Ans. 1\n",
        "\n",
        "1. For x~N(0,I) i.e x=(x1,x2,....xn)T  where xi are N(0,1) random variables.then E(x)=(E(x1),,,,E(xn))T = (0,,,,,,0)T => E[x]=0 \n",
        "\n",
        "  E[y]= E[Ax+μ]= μ +AE[x]= μ\n",
        "\n",
        "2. For x~N(0,I)  then E[(xi-E[xi])(xj-E[xj])]=E[xi.xj]= 1 (if i=j) & 0 (if i!=j)   \n",
        "\n",
        "                      ==> E[x.(x)T]=I\n",
        "\n",
        "   E[(y-μ).(y-μ)T]= E[(Ax).(Ax)T]= AE[x.(x)T](A)T = A.I.(A)T= A(A)T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q72CAEnnbGLj"
      },
      "source": [
        "# Fill out these functions\n",
        "\n",
        "def E_y(ys:np.array):\n",
        "    \"\"\"\n",
        "    ys: A batch of column vectors\n",
        "    return: Mean of the column vectors\n",
        "    \"\"\"\n",
        "    return np.mean(ys,axis=0)\n",
        "\n",
        "def E_yyT(ys:np.array):\n",
        "    \n",
        "    \"\"\"\n",
        "    ys: A batch of column vectors\n",
        "    return: Covariance matrix of the column vectors\n",
        "    \"\"\"\n",
        "    M=ys.shape[1]\n",
        "    ys=np.reshape(ys,(50000,M));\n",
        "    return np.cov(ys,rowvar=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjCxOmqdbGLm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "bf5dfa55-c5ed-4fc0-e9cf-0533cf1e3b42"
      },
      "source": [
        "# Code to test the above two functions. \n",
        "\n",
        "M=2\n",
        "\n",
        "# Observe how the matrix shapes are adjusted to allow for the vectorization.\n",
        "A = np.random.randint(low=1,high=10,size=(1,M,M))\n",
        "xs = np.random.randn(50000,M,1)\n",
        "mu = np.random.randint(low=0,high=10,size=(1,M,1))\n",
        "\n",
        "ys = A @ xs + mu\n",
        "# The following two column-vectors should be nearly identical\n",
        "print(\"Mean:\")\n",
        "print(E_y(ys))\n",
        "print(mu,end=\"\\n\\n\")\n",
        "\n",
        "\n",
        "# The following two matrices should be nearly identical\n",
        "print(\"Covariance:\")\n",
        "print(E_yyT(ys-mu))\n",
        "print(A[0]@A[0].T)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean:\n",
            "[[5.99705056]\n",
            " [0.99194652]]\n",
            "[[[6]\n",
            "  [1]]]\n",
            "\n",
            "Covariance:\n",
            "[[65.01380163 78.98194523]\n",
            " [78.98194523 96.92930433]]\n",
            "[[65 79]\n",
            " [79 97]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3lIVUddbGLt"
      },
      "source": [
        "[Q3] Given a covariance matrix $\\mathbf{C}$ and mean $\\mu$, generate N samples of $y$ using $\\mathbf{x} \\sim N(0, I_m)$ such that $\\mathbb{E}[\\mathbf{y}]=\\mu$ \n",
        "and $\\mathbb{E}[\\left(\\mathbf{y-\\mu}\\right)\\left(\\mathbf{y-\\mu}\\right)^\\top]=\\mathbf{C}$.\n",
        "\n",
        "Hint: See np.linalg.cholesky"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN4sYXysbGLt"
      },
      "source": [
        "def sample(C, mu, N):\n",
        "    \"\"\"\n",
        "    C: An MxM \"positive definite\"(?!?!) symmetric matrix\n",
        "    mu: An Mx1 column vector \n",
        "    return: A batch of column-vectors with Mean=mu and Covariance=C\n",
        "    \"\"\"\n",
        "    M = C.shape[0]\n",
        "    # Write code here\n",
        "    L = np.expand_dims(np.linalg.cholesky(C),axis=0)\n",
        "\n",
        "    u = np.random.rand(M*N)\n",
        "    v = np.random.rand(M*N) \n",
        "    func = np.vectorize(normal)\n",
        "    u = func(u,v) \n",
        "    u = u.reshape(N,M)\n",
        "\n",
        "    #u = np.random.normal(loc=0, scale=1, size=M*N).reshape(N,M)\n",
        "    u = np.expand_dims(u,axis=2)\n",
        "    y = np.matmul(L,u)+mu\n",
        "\n",
        "    return y\n",
        "\n",
        "def normal(u,v,mean=0,std=1):\n",
        "  #Box Muller\n",
        "  u1= 2.0 * math.pi * u\n",
        "  v1 = math.sqrt( -2.0*math.log(1.0 - v))\n",
        "  return v1 * math.sin(u1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "mJnHRLOnbGLx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "85e00cd8-87cf-4710-9f10-c90a0b829349"
      },
      "source": [
        "# Code to test the above function \n",
        "\n",
        "M = 5\n",
        "\n",
        "# The following ensures that C is a positive semi-definite symmetric matrix. Find out why! What happens if low=0?\n",
        "A = np.random.randint(low=1,high=10,size=(M,M))\n",
        "A[np.triu_indices(M,k=1)] = 0\n",
        "\n",
        "C = A@A.T\n",
        "\n",
        "mu = np.random.randint(low=0,high=10,size=(1,M,1))\n",
        "\n",
        "ys = sample(C, mu, 50000)\n",
        "# The following two column-vectors should be nearly identical\n",
        "print(\"Mean:\")\n",
        "print(E_y(ys))\n",
        "print(mu,end=\"\\n\\n\")\n",
        "\n",
        "# The following two matrices should be nearly identical\n",
        "print(\"Covariance:\")\n",
        "print(E_yyT(ys-mu))\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean:\n",
            "[[8.03183009]\n",
            " [1.02378772]\n",
            " [3.99629589]\n",
            " [0.94745071]\n",
            " [2.98847789]]\n",
            "[[[8]\n",
            "  [1]\n",
            "  [4]\n",
            "  [1]\n",
            "  [3]]]\n",
            "\n",
            "Covariance:\n",
            "[[ 64.18567273  56.46962725   8.12559536  16.1292677   64.38971797]\n",
            " [ 56.46962725  74.68895562  17.14565489  38.91244938  86.38882663]\n",
            " [  8.12559536  17.14565489   6.0363125   20.05871838  22.08989346]\n",
            " [ 16.1292677   38.91244938  20.05871838 175.2206143  134.43632733]\n",
            " [ 64.38971797  86.38882663  22.08989346 134.43632733 172.51932642]]\n",
            "[[ 64  56   8  16  64]\n",
            " [ 56  74  17  39  86]\n",
            " [  8  17   6  20  22]\n",
            " [ 16  39  20 174 134]\n",
            " [ 64  86  22 134 172]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDH3fh75bGL4"
      },
      "source": [
        "# II. Invertible Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_7itkw4bGL4"
      },
      "source": [
        "Invertible neural networks -- which learn bijective (invertable) functions $f$ such that if $y=f(x)$, we should be  able to obtain an unique $x=f^{-1}(y)$ -- play a central role in many applications. In this assignment, we will work with invertible fully-connected 3-Layer Neural Networks. \n",
        "\n",
        "Recall that the main operation in neural networks is matrix multiplication and in order to compute the inverse, we would need to invert the matrices. As computing inverse is costly, we store each matrix in their SVD factorized forms $W = USV$, where U and V are orthogonal matrices and S is diagonal [with none of the diagonal entries being 0]. Also, observe that the number of hidden units at all layers are equal to the number of input units (why?!).\n",
        "\n",
        "In this assignment, we will use parameters of an \"already-invertible\" neural network and just write the code for computing the forward and inversion operations. \n",
        "\n",
        "Write the code for forward pass, by choosing \"LeakyReLU\" as the activation function with negative_slope=0.1. Subsequently, write the code for the invert function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGHWS5VVbGL5"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class InvertibleNN(torch.nn.Module):\n",
        "    def __init__(self, m):\n",
        "        \n",
        "        super(InvertibleNN,self).__init__()\n",
        "                \n",
        "        self.u1 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.s1 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.v1 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.b1 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m)))\n",
        "        \n",
        "        self.u2 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.s2 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.v2 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.b2 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m)))\n",
        "        \n",
        "        self.u3 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.s3 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.v3 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m,m)))\n",
        "        self.b3 = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(m)))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: A batch input of shape (N,m)\n",
        "        return: The output of the fully-connected 3-layer neural network\n",
        "        \"\"\"\n",
        "        # Write code here\n",
        "        self.w1=self.u1 @ torch.diag(torch.reciprocal(self.s1)) @ torch.t(self.v1)\n",
        "        self.w2=self.u2 @ torch.diag(torch.reciprocal(self.s2)) @ torch.t(self.v2)\n",
        "        self.w3=self.u3 @ torch.diag(torch.reciprocal(self.s3)) @ torch.t(self.v3)\n",
        "        self.a1= torch.matmul(x,self.w1)+self.b1\n",
        "        self.a1 = F.leaky_relu(self.a1,negative_slope=0.1)\n",
        "        self.a2 = torch.matmul(self.a1,self.w2)+self.b2\n",
        "        self.a2 = F.leaky_relu(self.a2,negative_slope=0.1)\n",
        "        self.a3 = torch.matmul(self.a2,self.w3)+self.b3\n",
        "        self.y=F.leaky_relu(self.a3,negative_slope=0.1)\n",
        "        return self.y\n",
        "    \n",
        "    def invert(self, y):\n",
        "        \"\"\"\n",
        "        y: A batch input of shape (N,m)\n",
        "        return: The input of the fully-connected 3-layer neural network which would yield y\n",
        "        \"\"\"\n",
        "        \n",
        "        # Write code here\n",
        "        lrate = 0.1\n",
        "\n",
        "        y = y.reshape(-1,y.shape[0])\n",
        "        y *= lrelu_prime(self.y)\n",
        "        #w3_p = torch.matmul(torch.t(self.a2),y)\n",
        "        #b3_p = y.clone() \n",
        "        y3 = torch.matmul(y,torch.t(self.w3)) - self.b3\n",
        "        #y3 = y3.reshape(-1,1)\n",
        "        \n",
        "        y3 *= lrelu_prime(self.a2)\n",
        "        #w2_p = torch.matmul(torch.t(self.a1),y3)\n",
        "        #b2_p = y3.clone()\n",
        "        y2= torch.matmul(y3,torch.t(self.w2)) - self.b2\n",
        "        #y2 = y2.reshape(-1,1)\n",
        "        \n",
        "      \n",
        "        y2 *= lrelu_prime(self.a1)\n",
        "        #w1_p = torch.matmul(torch.t(x),y2)\n",
        "        #b1_p = y2.clone()\n",
        "        #d = x.squeeze(0)\n",
        "        y1 = torch.matmul(y2,torch.t(self.w1)) - self.b1\n",
        "\n",
        "        self.w3 += torch.matmul(torch.t(self.a2),y3) * lrate\n",
        "        self.w2 += torch.matmul(torch.t(self.a1),y2) * lrate\n",
        "        self.w1 += torch.matmul(x,y1) * lrate\n",
        "        #self.b3 += y3.sum() * lrate\n",
        "        #self.b2 += y2.sum() * lrate\n",
        "        #self.b1 += y1.sum() * lrate\n",
        "        return y1\n",
        "\n",
        "  \n",
        "def lrelu_prime(x,a=0.1):\n",
        "  return torch.where(x >= 0, torch.tensor(1.), torch.tensor(a))\n",
        "\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPIcnhXGbGL8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "6f356817-1974-447d-d860-8592e5c971a3"
      },
      "source": [
        "# Code to test the forward and invert functions\n",
        "\n",
        "\n",
        "for m in range(1,11):\n",
        "    iNN = InvertibleNN(m)\n",
        "    \n",
        "    r = requests.get(\"https://web.cs.dal.ca/~sastry/inv_net_{}.pt\".format(m))\n",
        "    iNN.load_state_dict(torch.load(io.BytesIO(r.content)))\n",
        "\n",
        "    x = torch.FloatTensor(np.random.randint(low=1,high=10,size=(1,m)))\n",
        "    # These 2 outputs should be nearly identical. \n",
        "    print(x)\n",
        "    print(iNN.invert(iNN(x)))\n",
        "    \n",
        "    print(\"=\"*40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7.]])\n",
            "tensor([-2.2257], grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[3., 2.]])\n",
            "tensor([-6.9207, -6.9207], grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[7., 7., 6.]])\n",
            "tensor([ -7.2828, -10.2828,  -9.2828], grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[7., 8., 2., 1.]])\n",
            "tensor([-6.0015, -5.0015, -7.0015, -4.0015], grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[4., 9., 4., 8., 9.]])\n",
            "tensor([-3.4592, -4.4592, -4.4592, -5.4592, -4.4592], grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[2., 3., 7., 4., 2., 1.]])\n",
            "tensor([ 8.9218, 10.9218,  7.9218, 13.9218,  7.9218,  6.9218],\n",
            "       grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[7., 1., 2., 8., 1., 5., 1.]])\n",
            "tensor([-1.8621,  2.1379, -2.8621, -0.8621, -3.8621, -5.8621,  1.1379],\n",
            "       grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[8., 8., 8., 3., 9., 8., 4., 6.]])\n",
            "tensor([-4.5493, -0.5493, -3.5493,  1.4507, -0.5493, -2.5493, -5.5493, -1.5493],\n",
            "       grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[8., 7., 8., 5., 5., 3., 6., 3., 6.]])\n",
            "tensor([-0.2130, -2.2130, -3.2130, -1.2130,  1.7870, -2.2130, -1.2130, -3.2130,\n",
            "        -3.2130], grad_fn=<SubBackward0>)\n",
            "========================================\n",
            "tensor([[2., 1., 7., 2., 4., 4., 9., 8., 2., 2.]])\n",
            "tensor([-4.3827, -3.3827, -3.3827, -1.3827, -9.3827, -6.3827, -1.3827, -4.3827,\n",
            "        -6.3827, -2.3827], grad_fn=<SubBackward0>)\n",
            "========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YEQKt3yapE-"
      },
      "source": [
        "[Bonus] In certain applications, it is important to calculate the trace of the input-output Jacobian matrix. Shown below is the code for computing the trace of the input-output jacobian. In general, for an N-dimensional input, we would require N `backward` passes [which can easily blow up in real-world settings]. However, we can reduce the number of backward passes by using the Huchtinson's trick:\n",
        "\n",
        "$$\\mathbb{E}[tr(A)]=\\mathbb{E}_{x}[x^\\top A x]$$\n",
        "\n",
        "where, $x$ is a random variable such that $\\mathbb{E}[x]=0$ and $\\mathbb{E}[xx^\\top] = I$. Popular choices are Rademacher and Gaussian Distributions. \n",
        "\n",
        "Can you use the Hutchinson's trick to estimate the trace of the input-output Jacobian? Your output should be almost identical to the exact trace for the suggested configuration of the Neural Network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3jyYRekaoNE"
      },
      "source": [
        "def exact_trace(iNN,x):\n",
        "    x.requires_grad = True\n",
        "    y = iNN(x)\n",
        "    sum = 0\n",
        "    for i in range(x.shape[1]):\n",
        "        sum += torch.autograd.grad(y[0,i],x,retain_graph=True)[0][0,i]\n",
        "    return sum\n",
        "\n",
        "def approximate_trace(iNN,x):\n",
        "    \n",
        "    return None\n",
        "\n",
        "iNN = InvertibleNN(1000)\n",
        "\n",
        "r = requests.get(\"https://web.cs.dal.ca/~sastry/inv_net_1000.pt\")\n",
        "iNN.load_state_dict(torch.load(io.BytesIO(r.content)))\n",
        "\n",
        "for i in range(10):\n",
        "    x = torch.FloatTensor(np.random.randn(1,1000))\n",
        "    print(exact_trace(iNN,x))\n",
        "    print(approximate_trace(iNN,x))\n",
        "    print(\"=\"*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCCLlzTJbGL-"
      },
      "source": [
        "# III. SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lnpdm1qbGL_"
      },
      "source": [
        "In this section, we will solve a simple linear regression problem, wherein we require x+y+z = 1. However, a particular application requires all the x,y and z values to be within 0 and 1 and the following code chooses x,y, and z values which are sometimes beyond the said limits. \n",
        "\n",
        "Without modifying the loss function, can you modify the below code such that the above-mentioned condition is satisfied? \n",
        "\n",
        "Hint: Apply a transformation F to p such that $0\\le F(p) \\le 1$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKMwL7WhbGL_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b455c84-95a2-427a-b9f3-5974803cb240"
      },
      "source": [
        "p = torch.nn.Parameter(torch.FloatTensor(np.random.randn(3)))\n",
        "it = 0\n",
        "\n",
        "while it<=1000:\n",
        "    loss = 0.5 * (p.sum()-1)**2\n",
        "    p = p - 0.01*torch.autograd.grad(loss,p)[0]\n",
        "    p = f.softmax(p,dim=0)\n",
        "    it += 1\n",
        "print(p.sum().item(),p.detach().numpy(),loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0 [0.33333334 0.33333334 0.33333334] 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}